{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe1102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your search parameters\n",
    "keywords = \"\"\"Data%20Engineer\"%20OR%20\"Data%20Scientist\"\"\"\n",
    "location = \"United%20States\"\n",
    "time_filter = \"r1800\"  # Posted within ~30minutes \n",
    "experience_level = \"2%2C3\"   #( %2C means comma in URL encoding 1=Internship, 2=Entry, 3=Associate ) # &f_E={experience_level}\n",
    "start = 0 # Starting position (first page)\n",
    "\n",
    "# Lists to store all data\n",
    "all_job_ids = []\n",
    "all_job_details = []\n",
    "\n",
    "# Pagination variables\n",
    "start = 0\n",
    "jobs_per_page = 10\n",
    "has_more_jobs = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1470e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "while has_more_jobs:\n",
    "    list_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={keywords}&location={location}&f_TPR={time_filter}&start={start}\"\n",
    "    \n",
    "    # Get page content\n",
    "    response = requests.get(list_url)\n",
    "    list_data = response.text\n",
    "    list_soup = BeautifulSoup(list_data, 'html.parser')\n",
    "    page_jobs = list_soup.find_all(\"li\")\n",
    "    \n",
    "    # If no jobs found on this page, exit the loop\n",
    "    if len(page_jobs) == 0:\n",
    "        has_more_jobs = False\n",
    "        print(f\"No more jobs found after position {start}\")\n",
    "        break\n",
    "    \n",
    "    # Get job IDs from current page\n",
    "    page_ids = []\n",
    "    for jobs in page_jobs:\n",
    "        base_card_div = jobs.find(\"div\", {\"class\": \"base-card\"})\n",
    "        if base_card_div and base_card_div.get(\"data-entity-urn\"):\n",
    "            job_id = base_card_div.get(\"data-entity-urn\").split(\":\")[-1]\n",
    "        print(f\"Found job ID: {job_id}\")\n",
    "        page_ids.append(job_id)\n",
    "    \n",
    "    # Add page IDs to overall list\n",
    "    all_job_ids.extend(page_ids)\n",
    "    \n",
    "    # Move to next page\n",
    "    start += jobs_per_page\n",
    "    \n",
    "    # Wait before next page request\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "    \n",
    "    # If fewer jobs than expected, we're on the last page\n",
    "    if len(page_ids) < jobs_per_page:\n",
    "        has_more_jobs = False\n",
    "        print(f\"Last page had {len(page_ids)} jobs instead of {jobs_per_page}, stopping pagination\")\n",
    "print(list_url)\n",
    "print(f\"Total job IDs found: {len(all_job_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c73616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_list = []\n",
    "\n",
    "for job_id in all_job_ids:  \n",
    "\n",
    "    # Sleep BEFORE making the request\n",
    "    time.sleep(random.uniform(0.5, 4.5))  # Random sleep to avoid being blocked\n",
    "    \n",
    "    job_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{job_id}\"\n",
    "    job_response = requests.get(job_url)\n",
    "    job_soup = BeautifulSoup(job_response.text, 'html.parser')\n",
    "    job_post = {}\n",
    "    # Extract company name\n",
    "    job_post[\"job_id\"] = job_id\n",
    "    \n",
    "    try:\n",
    "        job_post[\"company_name\"] = job_soup.find(\"a\", {\"class\": \"topcard__org-name-link topcard__flavor--black-link\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        job_post[\"company_name\"] = \"Not available\"\n",
    "    \n",
    "    # Extract job title\n",
    "    try:\n",
    "        job_post[\"job_title\"] = job_soup.find(\"h2\", {\"class\": \"top-card-layout__title\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        job_post[\"job_title\"] = \"Not available\"\n",
    "    \n",
    "    # Extract location\n",
    "    try:\n",
    "        job_post[\"location\"] = job_soup.find(\"span\", {\"class\": \"topcard__flavor topcard__flavor--bullet\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        job_post[\"location\"] = \"Not available\"\n",
    "    \n",
    "    # Extract time posted\n",
    "    try:\n",
    "        job_post[\"time_posted\"] = job_soup.find(\"span\", {\"class\": \"posted-time-ago__text\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        job_post[\"time_posted\"] = \"Not available\"\n",
    "    \n",
    "    # Extract job description\n",
    "    try:\n",
    "        description_div = job_soup.find(\"div\", {\"class\": \"description__text description__text--rich\"})\n",
    "        job_post[\"job_description\"] = description_div.text.strip()\n",
    "    except AttributeError:\n",
    "        job_post[\"job_description\"] = \"Not available\"\n",
    "\n",
    "    job_list.append(job_post)\n",
    "    print(f\"Processed job: {job_post['job_title']} at {job_post['company_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b85343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(job_list)\n",
    "\n",
    "# Display results\n",
    "print(f\"Total jobs collected: {len(df)}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f958514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV on your local machine\n",
    "df.to_csv(\"linkedin_jobs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
